{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 14| DNN(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "刻一個孿生網路來做數字辨識，什麼是孿生網路？\n",
    "1. https://zhuanlan.zhihu.com/p/29058453\n",
    "2. [one-shot learning](https://www.youtube.com/watch?v=r8LLorRACPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generator\n",
    "\n",
    "def MNIST_GEN(batch_size, X, y):\n",
    "    \"\"\"\n",
    "    given X and will return the anchor, compare and Y_label\n",
    "    anchor : size = [None, 28, 28]\n",
    "    compare : size = [None, 28, 28]\n",
    "    Y_label : size = [None, 1]\n",
    "    \"\"\"\n",
    "    ### shuffle images and labels ###\n",
    "    index_anchor = shuffle(range(len(X)))\n",
    "    index_compare = shuffle(range(len(X)))\n",
    "    X_anchor = X.take(index_anchor, axis = 0)\n",
    "    y_anchor = y.take(index_anchor, axis = 0)\n",
    "    X_compare = X.take(index_compare, axis = 0)\n",
    "    y_compare = y.take(index_compare, axis = 0)\n",
    "    \n",
    "    ### transfrom y, if image of anchor and compare is same, y = 0, else y = 1\n",
    "    y = (np.equal(y_anchor,y_compare)*-1) + 1\n",
    "    \n",
    "    for i in range(0, 60000, batch_size):\n",
    "        anchor = X_anchor[i:i+batch_size].reshape(batch_size, 28, 28, 1)/ 255.\n",
    "        compare = X_compare[i:i+batch_size].reshape(batch_size, 28, 28, 1)/ 255.\n",
    "        Y_label = y[i:i+batch_size].reshape(batch_size, 1)\n",
    "        yield anchor, compare, Y_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "kernel_size = (3,3)\n",
    "pooling_size = (2,2)\n",
    "learning_rate = 0.001\n",
    "margin_siamese = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_nn = tf.Graph()\n",
    "with siamese_nn.as_default():\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        anchor_img = tf.placeholder(dtype = tf.float32, shape = [None, 28, 28, 1], name = 'anchor_image')\n",
    "        compare_img = tf.placeholder(dtype = tf.float32, shape = [None, 28, 28, 1], name = 'compare_image')\n",
    "        Y_label = tf.placeholder(dtype = tf.int32, shape = [None, 1], name = 'Y_label')\n",
    "        learning_rate = tf.placeholder(dtype = tf.float32, shape = [])\n",
    "        \n",
    "    with tf.variable_scope('siamese_net'):\n",
    "        conv_s1_1 = tf.layers.conv2d(anchor_img, filters= 64, kernel_size= kernel_size, activation= tf.nn.relu, name= 'siamese_1')\n",
    "        conv_s1_1 = tf.layers.max_pooling2d(conv_s1_1, pool_size= pooling_size, strides = (1,1), name= 'siamese_1')\n",
    "        \n",
    "        conv_s1_2 = tf.layers.conv2d(conv_s1_1, filters = 32, kernel_size = kernel_size, activation= tf.nn.relu, name= 'siamese_2')\n",
    "        conv_s1_2 = tf.layers.max_pooling2d(conv_s1_2, pool_size = pooling_size, strides = (1,1), name= 'siamese_2')\n",
    "\n",
    "        conv_s1_3 = tf.layers.conv2d(conv_s1_2, filters = 16, kernel_size = kernel_size, activation= tf.nn.relu, name= 'siamese_3')\n",
    "        conv_s1_3 = tf.layers.max_pooling2d(conv_s1_3, pool_size = pooling_size, strides = (1,1), name= 'siamese_3')\n",
    "        \n",
    "        anchor_vector = tf.layers.flatten(conv_s1_3, name = 'flatten')\n",
    "    \n",
    "    with tf.variable_scope('siamese_net', reuse= True):\n",
    "        \n",
    "        conv_s2_1 = tf.layers.conv2d(compare_img, filters= 64, kernel_size= kernel_size, activation= tf.nn.relu, name= 'siamese_1')\n",
    "        conv_s2_1 = tf.layers.max_pooling2d(conv_s2_1, pool_size= pooling_size, strides = (1,1), name= 'siamese_1')\n",
    "        \n",
    "        conv_s2_2 = tf.layers.conv2d(conv_s2_1, filters = 32, kernel_size = kernel_size, activation= tf.nn.relu, name= 'siamese_2')\n",
    "        conv_s2_2 = tf.layers.max_pooling2d(conv_s2_2, pool_size = pooling_size, strides = (1,1), name= 'siamese_2')\n",
    "\n",
    "        conv_s2_3 = tf.layers.conv2d(conv_s2_2, filters = 16, kernel_size = kernel_size, activation= tf.nn.relu, name= 'siamese_3')\n",
    "        conv_s2_3 = tf.layers.max_pooling2d(conv_s2_3, pool_size = pooling_size, strides = (1,1), name= 'siamese_3')\n",
    "        \n",
    "        compare_vector = tf.layers.flatten(conv_s2_3, name = 'flatten')\n",
    "        \n",
    "    \n",
    "    with tf.name_scope('loss_function'):\n",
    "#         loss = tf.contrib.losses.metric_learning.contrastive_loss(Y_label, anchor_vector, compare_vector)\n",
    "        \n",
    "        l2_norm = tf.sqrt(tf.reduce_sum(tf.square(anchor_vector-compare_vector), 1, keep_dims=True))\n",
    "        Ls = tf.multiply(0.5, tf.pow(l2_norm,2))\n",
    "        Ld = tf.multiply(0.5, tf.pow(tf.math.maximum(tf.zeros(1), l2_norm),2))\n",
    "        loss = tf.reduce_sum(tf.multiply(tf.cast(1-Y_label, tf.float32), Ls) + tf.multiply(tf.cast(Y_label,tf.float32), Ld))\n",
    "        tf.summary.scalar(\"Siamese_loss\",loss)\n",
    "        \n",
    "    with tf.name_scope('training'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "        optimizer.minimize(loss)\n",
    "        \n",
    "    \n",
    "    merge = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(logdir= './Siamese_train', graph = siamese_nn)\n",
    "    test_writer = tf.summary.FileWriter(logdir= './Siamese_test', graph = siamese_nn)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph= villina_CNN)\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_bar = tqdm_notebook(range(epoch))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
